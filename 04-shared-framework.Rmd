# A shared framework for disparate spatial systems in R?

## Nesting

tidyr and gggeom show that nesting can be used to always work in a single table, and sp can be emulated with single-nesting of the fortify table, or more closely with double nesting, first on object and then on piece. gggeom keeps attributes in separate lists, in order to allow for different numbers of values in each (though tables can do that too . . .). 

Side notes about the difference between sp and sf, and the island/hole things versus parent for branches. 


The main issues with nesting into one table, with nested components, are that: 

* it doesn't allow for many-to-one de-duplication indexing (unless the nested component stores an index to another table - but that's not one  table). 

* it's not readily backed by a database

A system of normalized tables is ready for transfer to a database, and can be read directly from a database *without any specialist tools for special types*

## Decomposition to relational tables

We have seen that a wide variety of data configurations can be converted to a set of relational tables and that these give systematic and straightforward pathways for reconstructing other forms. 
This provides the opportunity for an API where specialized packages provide the special methods to convert from and to the shared form, and so many conversions then become automatic and easy. 

Can we create simple idioms to encode decomposition from recursive objects generally? The cascading semi-join makes for very simple propagation of a subset from teh object table down through the other tables, and a cascading inner join automatically builds the right spbabel/fortify table that can be used directly, or as a stepping stone to constructing recursive list forms. 

The tidy initiative has show that these high-level processes can be abstracted into commonly used tools, but so far it's  been about tidying up model outputs and reshaping between long and wide forms. 


spbabel is the fortify model with two tables
  : the geometry - object, part, hole, order, coordinates
  : the metadata - object-level attributes

mtable is the branch model of vertices, branches, objects

p1table is the line-primitive model of vertices, line segments, objects

p2table is the tri-primitive model of vertices, triangles, objects

Pathways 

sp -> spbabel 

mtable -> spbabel 

spbabel -> mtable

spbabel -> sp



## A tidy version of the "sf" simple features package

The idea is that we step one level down from sf, derive the raw geometry and data from rgdal2, and the push that out to sf, sp, ggplot2, ggvis/gggeom, and so on. 


```{r}
#devtools::install_github("edzer/rgdal2")

flatten <-  function(x) {
  if (all(c("x", "y") %in% names(x))) { # we're at the deepest level
    as_tibble(x)
  } else {
    lapply(x, flatten)
  }
} 
  
flatten(list(list(list(x = 1, y = 2), list(x = 2:1, y = 3:4)), 
        list(list(x = 1, y = 2), list(x = 2:1, y = 3:4))))

readFeature = function(layer, id) {
  ft = rgdal2::getFeature(layer, id)
  geom = rgdal2::getGeometry(ft)
  flatten(rgdal2::getPoints(geom, nested = TRUE))
}

#' simplification of sf::read.sf to avoid classes
#' need to return the data_data still
readgdal <- function(x, layer = 1L) {
  if (!requireNamespace("rgdal2", quietly = TRUE))
    stop("package rgdal2 required for this function; try devtools::install_github(\"edzer/rgdal2\")")
  o <- rgdal2::openOGRLayer(x, layer)
  ids <- rgdal2::getIDs(o)
  srs <- rgdal2::getSRS(o)
  p4s <- if (is.null(srs)) as.character(NA) else rgdal2::getPROJ4(srs)
  geom <-  lapply(ids, function(id) readFeature(o, id))
  return(geom)
#  f <- lapply(ids, function(id) rgdal2::getFields(rgdal2::getFeature(o, id)))
  
 # df = data.frame(row.names = ids, apply(do.call(rbind, f), 2, unlist))
}

library(rgdal2)
library(tibble)
library(dplyr)
f = system.file("example-data/continents", package = "rgdal2")

## raw geometry in recursive lists
x <- readgdal(f)
## convert to sp object (geometry only for now)
obj <- spbabel::sp(bind_rows(lapply(x, function(y) bind_rows(lapply(y, bind_rows, .id = "simple_feature"), .id = "part")), .id = "object") %>% 
  transmute(object_ = object, branch_ = part, island_ = simple_feature == 1, x_ = x, y_ = y, order_ = row_number()))

```

## Creating data structures 

I realize the dplyr verbs and ggplot approach already has everything, when you use the piped group_by and arrange all of the information is already there. 

We don't tend to get polygon data in fortify() form in the wild, but we always get animal tracking and GPS data in that form, and turning those into "line geometries" that also know the time stamp, depth, etc is a real missed opportunity - on CRAN you can see dozens of "trajectory" packages that really could use a more general approach.  

Imagine track data from multiple GPS tags each deployed on different objects : 

track_df %>% group_by(tag_id, deployment), %>% arrange(timestamp)  %>% linearize(x, y)

that would give a fully fledged "line" object having removed the repeated values for grouping, and all the remaining attributes sit on the vertices, but "x/y" and timestamp are now treated specially, similar to how the grouped_df carries this information along. 

Another imagined example for polygons: 

poly_df %>% group_by(id, part) %>% arrange(order) %>% polypatherize(long, lat)

Optionally add a driver at the end of the pipe chain like "sp()" or "sf()" or "geojson()" , "wkt()" to get the right format for a given application and it's looking like something handy. 


I'm trying to learn the tricks of NSE in dplyr and ggvis to see how this could work. It's obvious that things like geom_polygon() for visualization and "point in polygon" tests have a shared application. 

 I really think there's a grand vision here somewhere that can unify 



